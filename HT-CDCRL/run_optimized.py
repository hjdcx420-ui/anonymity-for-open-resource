import torch
import random
import numpy as np
import os
from pytorch_lightning import Trainer
import swanlab as wandb


# è®¾ç½®ä¸´æ—¶ç›®å½•åˆ°ç”¨æˆ·ç›®å½•ï¼Œé¿å… /tmp ç©ºé—´ä¸è¶³
os.environ['TMPDIR'] = '/home/'
os.makedirs('/home/', exist_ok=True)
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.callbacks.stochastic_weight_avg import StochasticWeightAveraging

# å‡è®¾æ‚¨çš„ä»£ç åº“åœ¨Pythonçš„æœç´¢è·¯å¾„ä¸­
# å¦‚æœä¸åœ¨ï¼Œæ‚¨å¯èƒ½éœ€è¦ä½¿ç”¨ sys.path.append()
from src.models.MAPPO_optimized import OptimizedMAPPO as MAPPO
from src.datamodules.MAPPODatamodule import MAPPODatamodule
from src.utils.logging_manager import LoggingManager

# ============== SwanLab é›†æˆï¼ˆLightning å›è°ƒ + è¿è¡Œåˆå§‹åŒ–ï¼‰ ==============
from pytorch_lightning.callbacks import Callback

class SwanLabLoggerCallback(Callback):
    """å°†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é˜¶æ®µçš„æŒ‡æ ‡åŒæ­¥åˆ° SwanLabã€‚"""

    def on_train_start(self, trainer, pl_module):
        # è®°å½•æ¨¡å‹è§„æ¨¡
        try:
            total_params = sum(p.numel() for p in pl_module.parameters())
            trainable_params = sum(p.numel() for p in pl_module.parameters() if p.requires_grad)
            wandb.log({
                "params/total": int(total_params),
                "params/trainable": int(trainable_params)
            })
        except Exception:
            pass

    def _to_scalar(self, value):
        """å°½é‡æŠŠå„ç§ç±»å‹(å¼ é‡ã€numpyã€torchmetrics)è½¬ä¸ºfloatã€‚å¤±è´¥åˆ™è¿”å›Noneã€‚"""
        try:
            # torchmetrics.Metric
            if hasattr(value, "compute") and callable(getattr(value, "compute")):
                value = value.compute()
            # torch.Tensor
            if hasattr(value, "detach"):
                value = value.detach().cpu()
            # numpy scalar
            import numpy as _np  # å±€éƒ¨å¯¼å…¥ï¼Œé¿å…é¡¶å±‚ä¾èµ–
            if hasattr(value, "item"):
                return float(value.item())
            if isinstance(value, (int, float)):
                return float(value)
            if isinstance(value, _np.ndarray) and value.size == 1:
                return float(value.reshape(()))
        except Exception:
            return None
        try:
            return float(value)
        except Exception:
            return None

    def _log_metrics(self, trainer, prefixes):
        try:
            metrics = {}
            for k, v in trainer.callback_metrics.items():
                if isinstance(k, str) and (not prefixes or any(k.startswith(p) for p in prefixes)):
                    scalar = self._to_scalar(v)
                    if scalar is not None:
                        metrics[k] = scalar
            if metrics:
                # è®© SwanLab è‡ªåŠ¨æ­¥è¿›
                wandb.log(metrics)
        except Exception:
            pass

    # è®­ç»ƒ/éªŒè¯/æµ‹è¯•åœ¨ epoch ç»“æŸæ—¶éƒ½æ‰“ç‚¹ï¼Œç¡®ä¿æ›²çº¿å‡ºç°
    def on_train_epoch_end(self, trainer, pl_module):
        self._log_metrics(trainer, ("train/",))

    def on_validation_epoch_end(self, trainer, pl_module):
        self._log_metrics(trainer, ("val/",))

    def on_test_epoch_end(self, trainer, pl_module):
        self._log_metrics(trainer, ("test/",))

    # è®­ç»ƒæ¯ä¸ªbatchç»“æŸä¹Ÿæ‰“ç‚¹ï¼Œç¡®ä¿æ›²çº¿å®æ—¶æ›´æ–°
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        self._log_metrics(trainer, ("train/",))

def get_optimized_config():
    """
    åŸºäºå®éªŒç»“æœä¼˜åŒ–çš„é…ç½®
    ä¸»è¦æ”¹è¿›ï¼š
    1. å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆhidden_dimï¼‰
    2. ä¼˜åŒ–å­¦ä¹ ç‡è°ƒåº¦
    3. æ”¹è¿›æ­£åˆ™åŒ–ç­–ç•¥
    4. å¢å¼ºå¥–åŠ±ä¿¡å·
    5. åŠ¨æ€å€™é€‰é›†ç­–ç•¥
    """
    
    # ä½¿ç”¨æ›´ä¿å®ˆçš„å€™é€‰é›†å¤§å°ä»¥æå‡æ—©æœŸHit@1/5
    candi_merchant_num = 15
    
    config = {
        "project_name": "poi_recommender_optimized",
        "experiment_name": f"optimized_candi_{candi_merchant_num}",
        "seed": 5521,

        # Trainer é…ç½® - ä¼˜åŒ–è®­ç»ƒç­–ç•¥
        "trainer": {
            "devices": 1,
            "max_epochs": 200,  # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œé…åˆæ›´å¼ºçš„æ—©åœ
            "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
            "log_every_n_steps": 25,  # æ›´é¢‘ç¹çš„æ—¥å¿—è®°å½•
            "gradient_clip_val": 1.0,  # é€‚åº¦æ”¾æ¾æ¢¯åº¦è£å‰ª
            "precision": "16-mixed",
            "accumulate_grad_batches": 2,  # æ¢¯åº¦ç´¯ç§¯ï¼Œç­‰æ•ˆå¢åŠ batch size
            "check_val_every_n_epoch": 2,  # æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´
        },

        # Model (MAPPO) é…ç½® - é‡ç‚¹ä¼˜åŒ–
        "model": {
            "data_dir": "data/NYC",
            "obs_dim": 256,  # ğŸ”¥ å¢åŠ è§‚å¯Ÿç»´åº¦ï¼Œæå‡è¡¨ç¤ºèƒ½åŠ›
            "topk": [1, 5, 10, 15],
            "batch_size": 48,  # æå‡ç¨³å®šæ€§
            "committee_weight": 0.8,  # æ›´å¼ºè°ƒå€™é€‰è´¨é‡
            "lr": 5e-4,  # é™ä½å­¦ä¹ ç‡ï¼Œå‡å°æ›´æ–°å™ªå£°
            "weight_decay": 5e-5,  # ç•¥å‡æ­£åˆ™é¿å…æ¬ æ‹Ÿåˆ
            "update_steps": 8,  # ç¨é™æ›´æ–°å¼ºåº¦ï¼Œæå‡ç¨³å®šæ€§
            "candi_merchant_num": candi_merchant_num,
            "cat_num": 399,
            
            # ğŸ”¥ ä¼˜åŒ–å¥–åŠ±å‡½æ•°å‚æ•°
            "reward_alpha": 18.0,   # æ›´å¼ºåœ°æ¿€åŠ±Hit@1
            "reward_beta": 4.0,     # å¢å¼ºranking/margin
            "reward_gamma": 0.25,   # ç¨é™å¤šæ ·æ€§
            
            # ğŸ”¥ ä¼˜åŒ–PPOå‚æ•°
            "clip_ratio": 0.12,     # ç•¥æ”¶ç´§ï¼Œç¨³å®šä¼˜åŒ–
            "seed": 42,
            
            # ğŸ”¥ æ’åºèåˆç³»æ•°
            "ranking_alpha": 0.55,
        },

        # Datamodule é…ç½® - æ•°æ®æ•ˆç‡ä¼˜åŒ–
        "datamodule": {
            "data_dir": "data/NYC",
            "batch_size": 48,       # ä¸modelä¿æŒä¸€è‡´
            "test_batch_size": 128, # ğŸ”¥ æµ‹è¯•æ—¶å¯ä»¥ç”¨æ›´å¤§batch
            "num_workers": 4,       # æé«˜æ•°æ®ååï¼ˆè‹¥ç¯å¢ƒå…è®¸ï¼‰
            "seed": 42,
        },

        # ç»Ÿä¸€æ—©åœé…ç½®ï¼ˆå…è®¸å¤–éƒ¨è¦†ç›–ï¼‰ï¼šä¾‹å¦‚ 5 è¡¨ç¤ºéªŒè¯ 5 æ¬¡æ— æå‡å³åœæ­¢
        "early_stopping_patience": 5,
    }
    
    return config

def get_optimized_callbacks(config):
    """ä¼˜åŒ–çš„å›è°ƒå‡½æ•°é…ç½®"""
    callbacks = []
    
    # ğŸ”¥ æ”¹è¿›çš„æ¨¡å‹æ£€æŸ¥ç‚¹ç­–ç•¥
    checkpoint_callback = ModelCheckpoint(
        dirpath="/data/liuy/experiments/checkpoints/optimized/",
        filename=f"{config['experiment_name']}-{{epoch:02d}}-{{val/Hit@10:.3f}}-{{val/NDCG@20:.3f}}",
        save_top_k=3,  # ä¿å­˜top3æ¨¡å‹
        save_last=True,
        verbose=True,
        monitor="val/Hit@10",  # ä¸»è¦ç›‘æ§Hit@10
        mode="max",
        auto_insert_metric_name=False,
        every_n_epochs=5,  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡ï¼Œå‡å°‘I/O
    )
    callbacks.append(checkpoint_callback)
    
    # ğŸ”¥ æ”¹è¿›çš„æ—©åœç­–ç•¥ - æ›´è€å¿ƒä½†æ›´ç²¾ç¡®
    early_stopping_callback = EarlyStopping(
        monitor="val/Hit@5",  # ç›‘æ§Hit@5ï¼Œè¾ƒä¸ºæ•æ„Ÿçš„æŒ‡æ ‡
        patience=config.get("early_stopping_patience", 25),  # ä»é…ç½®è¯»å–ï¼Œé»˜è®¤25
        mode="max", 
        verbose=True, 
        strict=True, 
        check_finite=True,
        min_delta=0.005,      # é™ä½æœ€å°æ”¹è¿›é˜ˆå€¼
        stopping_threshold=0.6,  # å¦‚æœHit@5è¾¾åˆ°60%å°±å¯ä»¥è€ƒè™‘åœæ­¢
    )
    callbacks.append(early_stopping_callback)
    
    # ğŸ”¥ å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='epoch')
    callbacks.append(lr_monitor)
    
    # ğŸ”¥ éšæœºæƒé‡å¹³å‡ - æå‡æœ€ç»ˆæ€§èƒ½
    swa_callback = StochasticWeightAveraging(swa_lrs=1e-4, swa_epoch_start=150)
    callbacks.append(swa_callback)
    
    # SwanLab æŒ‡æ ‡åŒæ­¥
    callbacks.append(SwanLabLoggerCallback())
    
    return callbacks

def main():
    """ä¼˜åŒ–ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å‚æ•°ä¼˜åŒ–å®éªŒ")
    
    # è·å–ä¼˜åŒ–é…ç½®
    config = get_optimized_config()
    
    # ğŸ”¥ åˆå§‹åŒ–è§„èŒƒçš„æ—¥å¿—ç®¡ç†å™¨ - ä¿å­˜åˆ° /data åˆ†åŒº
    logging_manager = LoggingManager(
        base_dir="/data/liuy/experiments/logs",
        dataset_name="NYC",  # ä»configä¸­è·å–
        model_name="OptimizedMAPPO",
        experiment_name=config.get("experiment_name", "default_exp")
    )
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    logging_manager.save_config(config)
    print(f"ğŸ“ æ—¥å¿—ç›®å½•: {logging_manager.run_dir}")
    
    # åˆå§‹åŒ– SwanLab è¿è¡Œ
    wandb_run = wandb.init(
        project=config.get("project_name", "poi_recommender"),
        name=config.get("experiment_name", None),
        config=config,
    )
    
    # è®¾ç½®éšæœºç§å­
    seed = config["seed"]
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ğŸ”¥ ä¼˜åŒ–CUDAè®¾ç½®
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # å®ä¾‹åŒ–ç»„ä»¶
    print("ğŸ“Š å®ä¾‹åŒ–æ•°æ®æ¨¡å—...")
    datamodule = MAPPODatamodule(**config["datamodule"])

    print("ğŸ§  å®ä¾‹åŒ–ä¼˜åŒ–æ¨¡å‹...")
    model = MAPPO(**config["model"], logging_manager=logging_manager)
    
    # ğŸ”¥ æ¨¡å‹ä¼˜åŒ–åæ‰“å°å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # Logger - ä¿å­˜åˆ° /data åˆ†åŒº
    logger = TensorBoardLogger("/data/liuy/experiments/tb_logs", name=config["experiment_name"])

    # è·å–ä¼˜åŒ–çš„å›è°ƒå‡½æ•°
    callbacks = get_optimized_callbacks(config)

    # å®ä¾‹åŒ–å¹¶å¯åŠ¨Trainer
    print("âš¡ å®ä¾‹åŒ–ä¼˜åŒ–è®­ç»ƒå™¨...")
    trainer = Trainer(
        **config["trainer"],
        logger=logger,
        callbacks=callbacks,
        #  æ·»åŠ è®­ç»ƒä¼˜åŒ–é€‰é¡¹
        enable_model_summary=True,
        enable_checkpointing=True,
        enable_progress_bar=True,
        detect_anomaly=False,  # ç”Ÿäº§ç¯å¢ƒå…³é—­å¼‚å¸¸æ£€æµ‹
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸ¯ å¼€å§‹ä¼˜åŒ–è®­ç»ƒ!")
    print(f"é…ç½®æ¦‚è¦:")
    print(f"  å€™é€‰é›†å¤§å°: {config['model']['candi_merchant_num']}")
    print(f"  è§‚å¯Ÿç»´åº¦: {config['model']['obs_dim']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['model']['batch_size']}")
    print(f"  å­¦ä¹ ç‡: {config['model']['lr']}")
    print(f"  æ›´æ–°æ­¥æ•°: {config['model']['update_steps']}")
    print(f"  å¥–åŠ±å‚æ•°: Î±={config['model']['reward_alpha']}, Î²={config['model']['reward_beta']}, Î³={config['model']['reward_gamma']}")
    
    trainer.fit(model=model, datamodule=datamodule)

    print("ğŸ§ª å¼€å§‹æµ‹è¯•!")
    test_results = trainer.test(model=model, datamodule=datamodule)
    
    print("âœ… ä¼˜åŒ–å®éªŒå®Œæˆ!")
    if test_results:
        print("æœ€ç»ˆæµ‹è¯•ç»“æœ:")
        for metric, value in test_results[0].items():
            print(f"  {metric}: {value:.4f}")
            try:
                wandb.log({f"test/{metric}": float(value)})
            except Exception:
                pass
    
    # ç»“æŸ SwanLab ä¼šè¯
    try:
        wandb.finish()
    except Exception:
        pass

    return test_results

if __name__ == "__main__":
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    if not os.path.exists("data/NYC"):
        print("âŒ é”™è¯¯ï¼šæ•°æ®ç›®å½• 'data/NYC' ä¸å­˜åœ¨ã€‚è¯·å°†æ•°æ®æ–‡ä»¶æ”¾åœ¨è¯¥ç›®å½•ä¸‹ã€‚")
        exit()

    if not os.path.exists("checkpoints/optimized"):
        os.makedirs("checkpoints/optimized", exist_ok=True)
    
    if not os.path.exists("logs"):
        os.makedirs("logs", exist_ok=True)

    # è¿è¡Œä¼˜åŒ–å®éªŒ
    results = main()


